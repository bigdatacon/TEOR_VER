{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FM6XV3IlabW_"
   },
   "source": [
    "# Урок 4. Деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYL_20_gabXB"
   },
   "source": [
    "## Деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6OcIFWiabXC"
   },
   "source": [
    "В этом уроке пойдет речь еще об одном популярном методе машинного обучения - _деревьях решений_. Это семейство алгоритмов значительно отличается от линейных моделей, но применяется также в задачах классификации и регрессии.\n",
    "\n",
    "Метод основан на известной структуре данных - деревьях, которые по сути представляют собой последовательные инструкции с условиями. Например, в обсуждаемой ранее задаче кредитного скоринга может быть следующий алгоритм принятия решения:\n",
    "\n",
    "1. Старше ли клиент 18 лет? Если да, то продолжаем, иначе отказываем в кредите.\n",
    "\n",
    "2. Превышает ли его заработок 50 тысяч рублей? Если да, то продолжаем, иначе отказываем в кредите.\n",
    "\n",
    "3. Были ли у клиента просроченные кредиты ранее? Если да, отказываем в кредите, иначе выдаем.\n",
    "\n",
    "В листьях (терминальных узлах) деревьев стоят значения целевой функции (прогноз), а в узлах - условия перехода, определяющие, по какому из ребер идти. Если речь идет о бинарных деревьях (каждый узел производит ветвление на две части), обычно, если условие в узле истинно, то происходит переход по левому ребру, если ложно, то по правому. Изобразим описанный выше алгоритм в виде дерева решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-asfUSaabXD"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3875c4f399fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# если дерево ниже не отображается, требуется установить библиотеку python-graphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDigraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# если дерево ниже не отображается, требуется установить библиотеку python-graphviz\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aifJQkhTabXG",
    "outputId": "3d1ff9d0-5fe0-4131-d9a0-e8cf2d63dd21"
   },
   "outputs": [],
   "source": [
    "dot = Digraph(node_attr={'shape': 'box'})\n",
    "\n",
    "dot.node('A', label='Клиент старше 18 лет?')\n",
    "dot.node('B', label='Превышает ли его заработок 50 тысяч рублей?')\n",
    "dot.node('C', label='Отказать')\n",
    "dot.node('D', label='Были ли у клиента просроченные кредиты ранее?')\n",
    "dot.node('E', label='Отказать')\n",
    "dot.node('F', label='Отказать')\n",
    "dot.node('G', label='Выдать')\n",
    "\n",
    "dot.edge('A', 'B', label='да')\n",
    "dot.edge('A', 'C', label='нет')\n",
    "dot.edge('B', 'D', label='да')\n",
    "dot.edge('B', 'E', label='нет')\n",
    "dot.edge('D', 'F', label='да')\n",
    "dot.edge('D', 'G', label='нет')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asbWiXeOabXM"
   },
   "source": [
    "В задачах машинного обучения чаще всего в вершинах прописываются максимально простые условия. Обычно это сравнение значения одного из признаков $x^{j}$ с некоторым заданным порогом $t$:\n",
    "\n",
    "$$[x^{j} \\leq t].$$\n",
    "\n",
    "Если решается задача классификации, конечным прогнозом является класс или распределение вероятностей классов. В случае регрессии прогноз в листе является вещественным числом.\n",
    "\n",
    "Большим плюсом деревьев является тот факт, что они легко интерпретируемы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNt1_4CWabXM"
   },
   "source": [
    "## Построение деревьев решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IR2W64axabXN"
   },
   "source": [
    "Деревья обладают и отрицательными качествами - в частности, они очень легко переобучаются. Легко построить дерево, в котором каждый лист будет соответствовать одному объекту обучающей выборки. Оно будет идеально подогнано под обучающую выборку, давать стопроцентный ответ на ней, но при этом не будет восстанавливать оригинальных закономерностей, и качество ответов на новых данных будет неудовлетворительным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2k5dCHAQabXO"
   },
   "source": [
    "В машинном обучении деревья строятся последовательно от корня к листьям (так называемый \"жадный\" способ). Вначале выбирается корень и критерий, по которому выборка разбивается на две. Затем то же самое делается для каждого из потомков этого корня и так далее до достаточного уровня ветвления. Задача состоит в выборе способа разбиения каждого из узлов, то есть в выборе значения порога, с которым будет сравниваться значение одного из признаков в каждом узле.\n",
    "\n",
    "Разбиение выбирается с точки зрения некоторого заранее заданного функционала качества $Q(X, j, t)$. Находятся наилучшие значения $j$ и $t$ для создания _предиката_ $[x^{j}<t]$. Параметры $j$ и $t$ можно выбирать перебором: признаков конечное число, а из всех возможных значений порога $t$ можно рассматривать только те, при которых получаются различные разбиения на две подвыборки, таким образом, различных значений параметра $t$ будет столько же, сколько различных значений признака $x^{j}$ в обучающей выборке.\n",
    "\n",
    "В каждой вершине производится проверка, не выполнилось ли некоторое условие останова (критерии останова рассмотрим далее), и если оно выполнилось, разбиение прекращается, и вершина объвляется листом, и он будет содержать прогноз.\n",
    "\n",
    "В задаче классификации это будет класс, к которому относится большая часть объектов из выборки в листе $X_{m}$\n",
    "\n",
    "$$a_{m} = \\text{argmax}_{y \\in Y} \\sum_{i \\in X_{m}}[y_{i}=y]$$\n",
    "\n",
    "или доля объектов определенного класса $k$, если требуется предсказать вероятности классов\n",
    "\n",
    "$$a_{mk} = \\frac{1}{|X_{m}|} \\sum_{i \\in X_{m}}[y_{i}=k].$$\n",
    "\n",
    "В случае регрессии можно в качестве ответа давать средний по выборке в листе\n",
    "\n",
    "$$a_{m} = \\frac{1}{|X_{m}|} \\sum_{i \\in X_{m}}y_{i}.$$\n",
    "\n",
    "После построения дерева может проводиться его _стрижка_ (pruning) - удаление некоторых вершин согласно некоторому подходу с целью понижения сложности модели и повышения обобщающей способности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SV4vq5FrabXP"
   },
   "source": [
    "За функционал качества при работе с деревом решений принимается функционал вида\n",
    "\n",
    "$$Q(X_{m}, j, t) = H(X_{m}) - \\frac{|X_{l}|}{|X_{m}|}H(X_{l}) - \\frac{|X_{r}|}{|X_{m}|}H(X_{r}),$$\n",
    "\n",
    "где $X_{m}$ - множество объектов, попавших в вершину на данном шаге, $X_{l}$ и $X_{r}$ - множества, попадающие в левое и правое поддерево, соответственно, после разбиения. $H(X)$ - _критерий информативности_. Он оценивает качество распределения объектов в подмножестве и тем меньше, чем меньше разнообразие ответов в $X$, соответственно, задача обучения состоит в его минимизации и, соответственно, максимизации $Q(X_{m}, j, t)$ на данном шаге. Последний, по сути, характеризует прирост качества на данном шаге.\n",
    "\n",
    "В формуле значения критериев информативности нормируются - домножаются на долю объектов, ушедших в соответствующее подмножество. Например, если у нас множество в узле разбилось на два подмножества размером в 9990 объектов и 10 объектов, но при этом в первом подмножестве все объекты будут принадлежать к одному классу (то есть иметь минимальное значение разброса), а во втором - к разным, то в целом разбиение будет считаться хорошим, так как подавляющее большинство отсортировано правильно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjRAYZGJabXP"
   },
   "source": [
    "### Критерий информативности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUZfaAV7abXQ"
   },
   "source": [
    "В случае регрессии разброс будет характеризоваться дисперсией, поэтому критерий информативности будет записан в виде\n",
    "\n",
    "$$H(X) = \\frac{1}{X}\\sum_{i\\in X}(y_{i} - \\bar{y}(X))^{2},$$\n",
    "\n",
    "где $\\bar{y}(X)$ - среднее значение ответа в выборке $X$:\n",
    "\n",
    "$$\\bar{y}(X) = \\frac{1}{|X|}\\sum_{i\\in X}y_{i}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-j5hnV_abXR"
   },
   "source": [
    "В задаче классификации есть несколько способов определить критерий информативности.\n",
    "\n",
    "Обозначим через $p_{k}$ долю объектов класса $k$ в выборке $X$:\n",
    "\n",
    "$$p_{k} = \\frac{1}{|X|}\\sum_{i\\in X}[y_{i} = k].$$\n",
    "\n",
    "$p_{k}$ будет характеризовать вероятность выдачи класса $k$.\n",
    "\n",
    "_Критерий Джини_ или _индекс Джини_ выглядит следующим образом:\n",
    "\n",
    "$$H(X) = \\sum^{K}_{k=1}p_{k}(1-p_{k}),$$\n",
    "\n",
    "где $K$ - количество классов в наборе данных $X$.\n",
    "\n",
    "Его минимум достигается когда все объекты в подмножестве относятся к одному классу, а максимум - при равном содержании объектов всех класов. Критерий информативности Джини можно интерпретировать как вероятность ошибки случайного классификатора.\n",
    "\n",
    "Еще один критерий информативности - _энтропийный критерий_. Он также называется _энтропией Шеннона_ и записывается как\n",
    "\n",
    "$$H(X) = - \\sum^{K}_{k=1}p_{k}\\text{log}_{2}p_{k}.$$\n",
    "\n",
    "Минимум энтропии также достигается когда все объекты относятся к одному класссу, а максимум - при равномерном распределении. Прирост информации есть утрата неопределенности (=уменьшение энтропии). Стоит отметить, что в формуле полагается, что $0\\text{log}_{2}0=0.$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQbZUf1OabXS"
   },
   "source": [
    "### Критерии останова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNr4Fc7tabXS"
   },
   "source": [
    "_Критерии останова_ - это критерии, которые показывают, нужно ли остановить процесс построения дерева. Правильный выбор критериев останова роста дерева может существенно повлиять на его качество. Существует большое количество возможных ограничений:\n",
    "\n",
    "- Ограничение максимальной глубины дерева. Этот критерий считается достаточно грубым, но хорошо зарекомендовавшим себя в построении композиций деревьев - когда несколько деревьев объединяются в один алгоритм.\n",
    "\n",
    "\n",
    "- Ограничение максимального количества листьев.\n",
    "\n",
    "\n",
    "- Ограничение минимального количества $n$ объектов в листе. При этом оно должно быть достаточным, чтобы построить надежный прогноз.\n",
    "\n",
    "\n",
    "- Останов в случае, когда все объекты в листе относятся к одному классу.\n",
    "\n",
    "\n",
    "- Требование улучшения функционала качества при разбиении на какую-то минимальную величину.\n",
    "\n",
    "Подбор оптимальных критериев - сложная задача, которая обычно решается методом кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vyLBxK_AabXT"
   },
   "source": [
    "### Обрезка деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gvdu6ypbabXU"
   },
   "source": [
    "В случае применения метода стрижки (обрезки, прунинга) деревьев использовать критерии останова необязательно, и можно строить переобученные деревья, затем снижая их сложность, удаляя листья по некоторому критерию (например, пока улучшается качество на отложенной выборке). Считается, что стрижка работает лучше, чем критерии останова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9Lc5HJnabXU"
   },
   "source": [
    "Одним из методов стрижки является _cost-complexity pruning_. Допустим, мы построили дерево, обозначенное как $T_{0}$. В каждом из листьев находятся объекты одного класса, и значение функционала ошибки $R(T)$ при этом будет минимально на $T_{0}$. Для борьбы с переобучением к нему добавляют \"штраф\" за размер дерева (аналогично регуляризации, рассмотренной нами в предыдущих уроках) и получают новый функционал $R_{\\alpha}(T)$:\n",
    "\n",
    "$$R_{\\alpha}(T) = R(T) + \\alpha|T|,$$\n",
    "\n",
    "где $|T|$ - число листьев в дереве, $\\alpha$ - некоторый параметр регуляризации. Таким образом если при построении дерева на каком-то этапе построения алгоритма ошибка будет неизменна, а глубина дерева увеличиваться, итоговый функционал, состоящий из их суммы, будет расти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSY2sT6CabXV"
   },
   "source": [
    "Однако стрижка деревьев обладает существенными минусами. В частности, она является очень трудоемкой процедурой. Например, она может требовать вычисления функционала качества на валидационной выборке на каждом шаге. К тому же, на данный момент одиночные деревья на практике почти не используются, а используются композиции деревьев, и в этом случае стрижка как метод борьбы с переобучением становится еще более сложным подходом. Обычно в такой ситуации достаточно использовать простые критерии останова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g5FCRiMOabXW"
   },
   "source": [
    "## CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMNNa6u1abXX"
   },
   "source": [
    "CART (Classification and regression trees) - первый из алгоритмов, состоящий в обычном последовательном построении дерева решений и придуманный в 1983 году. На первой итерации строятся все возможные разбиения исходного пространства на два и выбирается такое, при котором максимально выделен один из классов в одно подпространство. На следующих итерациях выбирается худший лист (с наибольшим разнообразием классов), и на нем проводится та же операция. Так продложается до достижения одного из критериев останова. \n",
    "\n",
    "Полученное дерево будет подогнано под обучающую выборку (переобучено), так что затем требуется его кросс-валидация или обрезка методом cost-complexity pruning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQbwzLsWabXY"
   },
   "source": [
    "В качестве функции оценки качества разбиения используется критерий Джини, который также может быть записан как\n",
    "\n",
    "$$H(X) = 1 - \\sum^{K}_{k=1}p_{k}^{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sWm0sO22abXZ"
   },
   "source": [
    "## Реализация дерева решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yZFTHXJabXa"
   },
   "source": [
    "Реализуем алгоритм алгоритм работы дерева решений своими руками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7Xi827dabXb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FZaieu8abXe"
   },
   "outputs": [],
   "source": [
    "# сгенерируем данные\n",
    "classification_data, classification_labels = datasets.make_classification(n_features = 2, n_informative = 2, \n",
    "                                                      n_classes = 2, n_redundant=0, \n",
    "                                                      n_clusters_per_class=1, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHpOr1k6abXg",
    "outputId": "156a871e-72bd-4187-eb96-c6688945044e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# визуализируем сгенерированные данные\n",
    "\n",
    "colors = ListedColormap(['red', 'blue'])\n",
    "light_colors = ListedColormap(['lightcoral', 'lightblue'])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(list(map(lambda x: x[0], classification_data)), list(map(lambda x: x[1], classification_data)), \n",
    "              c=classification_labels, cmap=colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duu45IJUabXi"
   },
   "outputs": [],
   "source": [
    "# Реализуем класс узла\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, index, t, true_branch, false_branch):\n",
    "        self.index = index  # индекс признака, по которому ведется сравнение с порогом в этом узле\n",
    "        self.t = t  # значение порога\n",
    "        self.true_branch = true_branch  # поддерево, удовлетворяющее условию в узле\n",
    "        self.false_branch = false_branch  # поддерево, не удовлетворяющее условию в узле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QY8oNtakabXl"
   },
   "outputs": [],
   "source": [
    "# И класс терминального узла (листа)\n",
    "\n",
    "class Leaf:\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels  # y_true\n",
    "        self.prediction = self.predict()  # y_pred\n",
    "        \n",
    "    def predict(self):\n",
    "        # подсчет количества объектов разных классов\n",
    "        classes = []  # сформируем списов\n",
    "        for label in self.labels:\n",
    "            if label not in classes:\n",
    "                classes.append(label)\n",
    "            else:\n",
    "                cotinue\n",
    "\n",
    "        #  найдем среднее значение объектов в классе  \n",
    "        prediction = sum(сlasses) / len(classes)\n",
    "        return prediction        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBdli3WDabXn"
   },
   "outputs": [],
   "source": [
    "# Расчет критерия Джини\n",
    "\n",
    "def gini(labels):\n",
    "    #  подсчет количества объектов разных классов\n",
    "    classes = []\n",
    "    for label in labels:\n",
    "        if label not in classes:\n",
    "            classes.append(label)\n",
    "        else:\n",
    "            cotinue\n",
    "    \n",
    "    #  расчет критерия\n",
    "    impurity = 1     # \"impurity\" - \"нечистота\", степень неопределенности\n",
    "    for label in classes:\n",
    "        impurity -= np.std(classes)\n",
    "        \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vvLDhuRabXr"
   },
   "outputs": [],
   "source": [
    "# Расчет качества\n",
    "\n",
    "def quality(left_labels, right_labels, current_gini):\n",
    "\n",
    "    # доля выборки, ушедшей в левое поддерево\n",
    "    p = float(left_labels.shape[0]) / (left_labels.shape[0] + right_labels.shape[0])\n",
    "    \n",
    "    return current_gini - p * gini(left_labels) - (1 - p) * gini(right_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhWkphi3abXt"
   },
   "outputs": [],
   "source": [
    "# Разбиение датасета в узле\n",
    "\n",
    "def split(data, labels, index, t):\n",
    "    \n",
    "    left = np.where(data[:, index] <= t)\n",
    "    right = np.where(data[:, index] > t)\n",
    "        \n",
    "    true_data = data[left]\n",
    "    false_data = data[right]\n",
    "    true_labels = labels[left]\n",
    "    false_labels = labels[right]\n",
    "        \n",
    "    return true_data, false_data, true_labels, false_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wi8Fbs_abXw"
   },
   "outputs": [],
   "source": [
    "# Нахождение наилучшего разбиения\n",
    "\n",
    "def find_best_split(data, labels):\n",
    "    \n",
    "    #  обозначим минимальное количество объектов в узле\n",
    "    min_leaf = 5\n",
    "\n",
    "    current_gini = gini(labels)\n",
    "\n",
    "    best_quality = 0\n",
    "    best_t = None\n",
    "    best_index = None\n",
    "#     для количества листьев\n",
    "    count_list= []\n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    for index in range(n_features):\n",
    "        t_values = [row[index] for row in data]\n",
    "        \n",
    "        for t in t_values:\n",
    "            ### Количество листьев не более 50\n",
    "            while len(count_list) < 50:\n",
    "    \n",
    "                true_data, false_data, true_labels, false_labels = split(data, labels, index, t)\n",
    "                #  пропускаем разбиения, в которых в узле остается менее 5 объектов\n",
    "    \n",
    "                if len(true_data) < min_leaf or len(false_data) < min_leaf:\n",
    "                    continue\n",
    "\n",
    "                current_quality = quality(true_labels, false_labels, current_gini)\n",
    "\n",
    "                #  выбираем порог, на котором получается максимальный прирост качества\n",
    "                if current_quality > best_quality:\n",
    "                    best_quality, best_t, best_index = current_quality, t, index\n",
    "                count_list.append(true_data)\n",
    "\n",
    "    return best_quality, best_t, best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1__qSXSabXy"
   },
   "outputs": [],
   "source": [
    "# Построение дерева с помощью рекурсивной функции\n",
    "\n",
    "def build_tree(data, labels):\n",
    "\n",
    "    quality, t, index = find_best_split(data, labels)\n",
    "\n",
    "    #  Базовый случай - прекращаем рекурсию, когда нет прироста в качества\n",
    "    if quality == 0:\n",
    "        return Leaf(data, labels)\n",
    "\n",
    "    true_data, false_data, true_labels, false_labels = split(data, labels, index, t)\n",
    "\n",
    "    # Рекурсивно строим два поддерева\n",
    "    true_branch = build_tree(true_data, true_labels)\n",
    "    false_branch = build_tree(false_data, false_labels)\n",
    "\n",
    "    # Возвращаем класс узла со всеми поддеревьями, то есть целого дерева\n",
    "    return Node(index, t, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7ruKazCabX3"
   },
   "outputs": [],
   "source": [
    "# Проход объекта по дереву для его классификации\n",
    "\n",
    "def classify_object(obj, node):\n",
    "\n",
    "    #  Останавливаем рекурсию, если достигли листа\n",
    "    if isinstance(node, Leaf):\n",
    "        answer = node.prediction\n",
    "        return answer\n",
    "\n",
    "    if obj[node.index] <= node.t:\n",
    "        return classify_object(obj, node.true_branch)\n",
    "    else:\n",
    "        return classify_object(obj, node.false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLOdTAqdabX7"
   },
   "outputs": [],
   "source": [
    "# Предсказание деревом для всего датасета\n",
    "\n",
    "def predict(data, tree):\n",
    "    \n",
    "    classes = []\n",
    "    for obj in data:\n",
    "        prediction = classify_object(obj, tree)\n",
    "        classes.append(prediction)\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnDPkwVJabYA"
   },
   "outputs": [],
   "source": [
    "# Разобьем выборку на обучающую и тестовую\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = model_selection.train_test_split(classification_data, \n",
    "                                                                                     classification_labels, \n",
    "                                                                                     test_size = 0.3,\n",
    "                                                                                     random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbbvkqvOabYD"
   },
   "outputs": [],
   "source": [
    "# Построим дерево по обучающей выборке\n",
    "my_tree = build_tree(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GF1rVs9VabYG",
    "outputId": "2da01ba5-e2df-40f9-f613-a34bfc7733d0"
   },
   "outputs": [],
   "source": [
    "# Напечатаем ход нашего дерева\n",
    "def print_tree(node, spacing=\"\"):\n",
    "\n",
    "    # Если лист, то выводим его прогноз\n",
    "    if isinstance(node, Leaf):\n",
    "        print(spacing + \"Прогноз:\", node.prediction)\n",
    "        return\n",
    "\n",
    "    # Выведем значение индекса и порога на этом узле\n",
    "    print(spacing + 'Индекс', str(node.index))\n",
    "    print(spacing + 'Порог', str(node.t))\n",
    "\n",
    "    # Рекурсионный вызов функции на положительном поддереве\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "    # Рекурсионный вызов функции на положительном поддереве\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"  \")\n",
    "    \n",
    "print_tree(my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyvzFQp3abYI"
   },
   "outputs": [],
   "source": [
    "# Получим ответы для обучающей выборки \n",
    "train_answers = predict(train_data, my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rl6IyN8yabYN"
   },
   "outputs": [],
   "source": [
    "# И получим ответы для тестовой выборки\n",
    "answers = predict(test_data, my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2odXAe7vabYP"
   },
   "outputs": [],
   "source": [
    "# Введем функцию подсчета точности как доли правильных ответов\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jn0L6r8CabYS",
    "outputId": "901a6dd3-6e6a-4d8a-e019-0c09ad410dc0"
   },
   "outputs": [],
   "source": [
    "# Точность на обучающей выборке\n",
    "train_accuracy = accuracy_metric(train_labels, train_answers)\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D__eGtLRabYU",
    "outputId": "65b8c452-d407-4121-f159-e4c8040970fa"
   },
   "outputs": [],
   "source": [
    "# Точность на тестовой выборке\n",
    "test_accuracy = accuracy_metric(test_labels, answers)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEHbV9hXabYW",
    "outputId": "fd90ddf4-88a0-4721-8681-c79bdfb405d8"
   },
   "outputs": [],
   "source": [
    "# Визуализируем дерево на графике\n",
    "\n",
    "def get_meshgrid(data, step=.05, border=1.2):\n",
    "    x_min, x_max = data[:, 0].min() - border, data[:, 0].max() + border\n",
    "    y_min, y_max = data[:, 1].min() - border, data[:, 1].max() + border\n",
    "    return np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
    "\n",
    "plt.figure(figsize = (16, 7))\n",
    "\n",
    "# график обучающей выборки\n",
    "plt.subplot(1,2,1)\n",
    "xx, yy = get_meshgrid(train_data)\n",
    "mesh_predictions = np.array(predict(np.c_[xx.ravel(), yy.ravel()], my_tree)).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, mesh_predictions, cmap = light_colors)\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c = train_labels, cmap = colors)\n",
    "plt.title(f'Train accuracy={train_accuracy:.2f}')\n",
    "\n",
    "# график тестовой выборки\n",
    "plt.subplot(1,2,2)\n",
    "plt.pcolormesh(xx, yy, mesh_predictions, cmap = light_colors)\n",
    "plt.scatter(test_data[:, 0], test_data[:, 1], c = test_labels, cmap = colors)\n",
    "plt.title(f'Test accuracy={test_accuracy:.2f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DC48TQN9abYY"
   },
   "source": [
    "Как видно, дерево строит кусочно-постоянную разделяющую гиперплоскость, то есть состоящую из прямых, параллельных осям. Чем глубже дерево, тем сложнее гиперплоскость. Также происходит и в случае регрессии - график зависимости целевого значения восстанавливается кусочно-постоянной функцией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_tree = DecisionTreeClassifier(random_state=2020).fit(classification_data, classification_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(sk_tree, out_file='tree2.dot', \n",
    "#                 feature_names = classification_data.columns,\n",
    "                class_names = ['0', '1'],\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# path_to_graphviz = '/home/n-bar/anaconda3/lib/python3.7/site-packages/graphviz' # your path to graphviz (C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\ for example) \n",
    "# os.environ[\"PATH\"] += os.pathsep + path_to_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree2.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAYfxLSTabYZ"
   },
   "source": [
    "## Работа деревьев в случае пропущенных значений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhyvKAv2abYa"
   },
   "source": [
    "Иногда в реальных задачах бывает так, что не для всех объектов известно значение того или иного признака. Одним из преимуществ деревьев решений является возможность обрабатывать такие случаи.\n",
    "\n",
    "Допустим, требуется вычислить функционал качества для разбиения $[x_{j}<t]$, но в выборке $X_{m}$ для некоторого подмножества объектов $V_{j}$ неизвестно значение $j$-го признака. В этом случае функционал качества рассчитывается без учета этих объектов (обозначим выборку без их учета как $X_{m}\\text{\\ }V_{j}$), с поправкой на потерю информации:\n",
    "\n",
    "$$Q_{X_{m}, j, t} = \\frac{|X_{m}\\text{\\ } V_{j}|}{|X_{m}|}Q(X_{m}\\text{\\ }V_{j}, j,t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9mAVLqrabYb"
   },
   "source": [
    "Если такое разбиение окажется лучшим, объекты из $V_{j}$ помещаются в оба образованных поддерева.\n",
    "\n",
    "На этапе применения дерева выполняется похожая операция. Если объект попал в вершину, в которой нельзя вычислить критерий разбиения из-за отсутствия значения необходимого признака, прогнозы для него вычисляются в обоих поддеревьях, а затем усредняются с весами, пропорциональными числу объектов в них.\n",
    "\n",
    "$$\\frac{|X_{l}|}{|X_{m}|}a_{l}(x) + \\frac{|X_{r}|}{|X_{m}|}a_{r}(x),$$\n",
    "\n",
    "где $a$ - прогноз вероятности отнесения объекта $x$ к одному из классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMUVpzQ-abYd"
   },
   "source": [
    "Кроме этого подхода существует метод построения _суррогатных предикатов_ в каждой вершине. Проще говоря, это запасной предикат, который использует другой признак, но при этом дает максимально близкое к исходному разбиение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyKBrnbpabYd"
   },
   "source": [
    "## Работа деревьев с категориальными признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WaIif9BAabYe"
   },
   "source": [
    "Кроме вещественных и бинарных признаков в задаче могут иметь место категориальные признаки (делящиеся на конечное число категорий, например, цвета автомобилей). Самый простой способ учета категориальных признаков в алгоритме деревьев состоит в разбитии вершины на столько поддеревьев, сколько имеется возможных значений признака. В этом случае дерево называется _n-арным_. Условие разбиения будет простым (отнесение признака к той или иной категории), однако здесь появляется риск получения конечного дерева с очень большим числом листьев. В случае такого дерева критерий ошибки $Q$ будет состоять из $n$ слагаемых (или из $(n+1)$) в случае максимизируемого критерия, который мы использовали."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oo6IMSS-abYf"
   },
   "source": [
    "Есть и другой подход, заключающийся в формировании бинарных деревьев путем разделения множества значений признака $C = \\{c_{1}, ...,c_{n}\\}$ на два непересекающихся подмножества $C_{1}$ и $C_{2}$. После такого разделения условием разбиения в узле будет проверка принадлежности признака одному из подмножеств $[x \\in C_{1}]$.\n",
    "\n",
    "Задача остается в выборе оптимального варианта разбиения исходного множества на два подмножества, так как обычный перебор всех вариантов может быть крайне затруднительным из-за большого количества вариантов разбиения. В случаях с бинарной классификацией и регрессией используют следующий метод: все возможные значения категориального признака сортируются по определенному принципу, затем заменяются на натуральные числа.\n",
    "\n",
    "В случае бинарной классификации признаки упорядочиваются на основе того, какая доля объектов с такими признаками относится к классу +1. Если обозначить множество объектов в узле $m$, у которых $j$-й признак имеет значение $с$, через $X_{m}(c)$, а через $N_{m}(c)$ количество таких объектов, получим:\n",
    "\n",
    "$$\\frac{1}{N_{m}(c_{1})} \\sum_{x \\in X_{m}(c_{1})}[y_{i}=+1]\\leq...\\leq \\frac{1}{N_{m}(c_{n})} \\sum_{x \\in X_{m}(c_{n})}[y_{i}=+1],$$\n",
    "\n",
    "и после замены категории $c_{i}$ на натуральное число ищется разбиение как для вещественного признака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb3tpXPcabYf"
   },
   "source": [
    "В случае задачи регрессии сортировка происходит схожим образом, но вместо доли объектов положительного класса среди объектов с таким значением признака вычисляется средний ответ по объектам с соответствующим значением категориального признака:\n",
    "\n",
    "$$\\frac{1}{N_{m}(c_{1})} \\sum_{x \\in X_{m}(c_{1})}y_{i}\\leq...\\leq \\frac{1}{N_{m}(c_{n})} \\sum_{x \\in X_{m}(c_{n})}y_{i}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuInidP3abYh"
   },
   "source": [
    "## Дополнительные материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlxrQi3-abYh"
   },
   "source": [
    "1. [Энтропия](https://habr.com/ru/post/305794/)\n",
    "2. [Энтропия - теоретическое обоснование](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F)\n",
    "3. [Неопределенность Джини](https://dyakonov.org/2015/12/15/знакомьтесь-джини/)\n",
    "4. [Cost-Complexity Pruning](http://mlwiki.org/index.php/Cost-Complexity_Pruning)\n",
    "5. [Реализация дерева решений в функциональном стиле](https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb)\n",
    "6. [ООП-реализация дерева решений](https://github.com/curiousily/Machine-Learning-from-Scratch/blob/master/3_decision_trees.ipynb)\n",
    "7. [Пример работы дерева решений в задаче регрессии](https://habr.com/ru/company/ods/blog/322534/#derevo-resheniy-v-zadache-regressii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Решающее дерево - последовательное построение узлов, разбивающих множество входящих объектов согласно принципу минимизации “нечистоты” (impurity) узла\n",
    "* Предикаты для разбиения в узле выбираются из всего множества признаков\n",
    "* Деревья могут легко переобучиться под выборку, если не ограничивать их глубину\n",
    "* Деревья очень чувствительны к небольшим изменениям в выборке (шумам)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Нужно ли нормировать признаки при построении дерева решений?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Как повлияет на построение дерева линейное преобразование части признаков?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/L4_Q2.png\" style=\"width: 350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Может ли дерево решений для классификации, построенное без ограничения глубины, показывать качество меньше 1.0 на обучающей выборке?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/L4_Q3.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Представим, что в выборке есть коррелирующие признаки. Как это отразится на построении дерева решений?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/L4_Q4.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - у дерева решений:  \n",
    "1) низкий variance, но высокий bias  \n",
    "2) низкий variance и bias  \n",
    "**3) низкий bias, но высокий variance**  \n",
    "4) высокий variance и bias  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - В коде из методички реализуйте один или несколько из критериев останова (количество листьев, количество используемых признаков, глубина дерева и т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Реализуйте дерево для задачи регрессии. Возьмите за основу дерево, реализованное в методичке, заменив механизм предсказания в листе на взятие среднего значения по выборке, и критерий Джини на дисперсию значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - В коде из методички реализуйте один или несколько из критериев останова (количество листьев, количество используемых признаков, глубина дерева и т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Добавлен списко count_list для проверки количества листов <50\n",
    "\n",
    "\n",
    "# Нахождение наилучшего разбиения\n",
    "\n",
    "def find_best_split(data, labels):\n",
    "    \n",
    "    #  обозначим минимальное количество объектов в узле\n",
    "    min_leaf = 5\n",
    "\n",
    "    current_gini = gini(labels)\n",
    "\n",
    "    best_quality = 0\n",
    "    best_t = None\n",
    "    best_index = None\n",
    "#     для количества листьев\n",
    "    count_list= []\n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    for index in range(n_features):\n",
    "        t_values = [row[index] for row in data]\n",
    "        \n",
    "        for t in t_values:\n",
    "            ### Количество листьев не более 50\n",
    "            while len(count_list) < 50:\n",
    "    \n",
    "                true_data, false_data, true_labels, false_labels = split(data, labels, index, t)\n",
    "                #  пропускаем разбиения, в которых в узле остается менее 5 объектов\n",
    "    \n",
    "                if len(true_data) < min_leaf or len(false_data) < min_leaf:\n",
    "                    continue\n",
    "\n",
    "                current_quality = quality(true_labels, false_labels, current_gini)\n",
    "\n",
    "                #  выбираем порог, на котором получается максимальный прирост качества\n",
    "                if current_quality > best_quality:\n",
    "                    best_quality, best_t, best_index = current_quality, t, index\n",
    "                count_list.append(true_data)\n",
    "\n",
    "    return best_quality, best_t, best_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Реализуйте дерево для задачи регрессии. Возьмите за основу дерево, реализованное в методичке, заменив механизм предсказания в листе на взятие среднего значения по выборке, и критерий Джини на дисперсию значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### В коде выше где Labels добавляются в список classes и считается geni"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
